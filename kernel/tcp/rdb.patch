--- linux-2.6.35/include/linux/sysctl.h	2010-08-02 06:11:14.000000000 +0800
+++ linux-2.6.35-rdb/include/linux/sysctl.h	2012-03-07 10:18:04.206424215 +0800
@@ -339,6 +339,10 @@
 	NET_IPV4_ROUTE=18,
 	NET_IPV4_FIB_HASH=19,
 	NET_IPV4_NETFILTER=20,
+//hmy add
+        NET_IPV4_TCP_FORCE_THIN_RDB=29,         /*  Added @ Simula */
+        NET_IPV4_TCP_RDB_MAX_BUNDLE_BYTES=32,   /*  Added @ Simula */
+//hmy add
 
 	NET_IPV4_TCP_TIMESTAMPS=33,
 	NET_IPV4_TCP_WINDOW_SCALING=34,
--- linux-2.6.35/include/linux/tcp.h	2010-08-02 06:11:14.000000000 +0800
+++ linux-2.6.35-rdb/include/linux/tcp.h	2012-03-07 10:29:53.926388756 +0800
@@ -105,6 +105,9 @@
 #define TCP_COOKIE_TRANSACTIONS	15	/* TCP Cookie Transactions */
 #define TCP_THIN_LINEAR_TIMEOUTS 16      /* Use linear timeouts for thin streams*/
 #define TCP_THIN_DUPACK         17      /* Fast retrans. after 1 dupack */
+//hmy add
+#define TCP_THIN_RDB            18
+//hmy add
 
 /* for TCP_INFO socket option */
 #define TCPI_OPT_TIMESTAMPS	1
@@ -363,6 +366,7 @@
 	u32	snd_up;		/* Urgent pointer		*/
 
 	u8	keepalive_probes; /* num of allowed keep alive probes	*/
+	u8	thin_rdb; /*  hmy add */
 /*
  *      Options received (usually on last packet, some only on SYN packets).
  */
--- linux-2.6.35/include/net/tcp.h	2012-03-08 15:58:37.000000000 +0800
+++ linux-2.6.35-rdb/include/net/tcp.h	2012-03-07 10:30:05.410387599 +0800
@@ -195,6 +195,8 @@
 #define TCP_NAGLE_OFF		1	/* Nagle's algo is disabled */
 #define TCP_NAGLE_CORK		2	/* Socket is corked	    */
 #define TCP_NAGLE_PUSH		4	/* Cork is overridden for already queued data */
+#define TCP_FORCE_THIN_RDB		0 /* hmy add */
+#define TCP_RDB_MAX_BUNDLE_BYTES	0 /* hmy add */
 
 /* TCP thin-stream limits */
 #define TCP_THIN_LINEAR_RETRIES 6       /* After 6 linear retries, do exp. backoff */
@@ -202,6 +204,8 @@
 extern struct inet_timewait_death_row tcp_death_row;
 
 /* sysctl variables for tcp */
+extern int sysctl_tcp_force_thin_rdb; /* hmy add */
+extern int sysctl_tcp_rdb_max_bundle_bytes; /* hmy add */
 extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
--- linux-2.6.35/net/ipv4/sysctl_net_ipv4.c	2010-08-02 06:11:14.000000000 +0800
+++ linux-2.6.35-rdb/net/ipv4/sysctl_net_ipv4.c	2012-03-07 10:34:31.306375565 +0800
@@ -120,6 +120,22 @@
 }
 
 static struct ctl_table ipv4_table[] = {
+	/* hmy add */
+	       {       /*  Added @ Simula for thin streams */
+                .procname       = "tcp_force_thin_rdb",
+                .data           = &sysctl_tcp_force_thin_rdb,
+                .maxlen         = sizeof(int),
+                .mode           = 0644,
+                .proc_handler   = &proc_dointvec
+        },
+       {       /*  Added @ Simula for thin streams */
+                .procname       = "tcp_rdb_max_bundle_bytes",
+                .data           = &sysctl_tcp_rdb_max_bundle_bytes,
+                .maxlen         = sizeof(int),
+                .mode           = 0644,
+                .proc_handler   = &proc_dointvec
+        },
+	/*  hmy add end */
 	{
 		.procname	= "tcp_timestamps",
 		.data		= &sysctl_tcp_timestamps,
--- linux-2.6.35/net/ipv4/tcp.c	2012-03-08 15:58:37.000000000 +0800
+++ linux-2.6.35-rdb/net/ipv4/tcp.c	2012-03-14 10:42:58.417678017 +0800
@@ -278,7 +278,12 @@
 #include <asm/ioctls.h>
 
 int sysctl_tcp_fin_timeout __read_mostly = TCP_FIN_TIMEOUT;
+/*  hmy add */
 
+int sysctl_tcp_force_thin_rdb __read_mostly = TCP_FORCE_THIN_RDB;
+int sysctl_tcp_rdb_max_bundle_bytes __read_mostly = TCP_RDB_MAX_BUNDLE_BYTES;
+
+/* hmy add end */
 struct percpu_counter tcp_orphan_count;
 EXPORT_SYMBOL_GPL(tcp_orphan_count);
 
@@ -902,6 +907,166 @@
 	return tmp;
 }
 
+
+/*  对于thin stream的tcp应用，发包的时候，如果上一个包的数据还没被确认，把上一个包的内容冗余到当前包里面发送,当上一个包有丢失的时候，避免重传造成的延时*/
+
+/*  hmy add */
+
+/*   Added at Simula to support RDB */
+static int tcp_trans_merge_prev(struct sock *sk, struct sk_buff *skb, int mss_now)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	
+	/*   Make sure that this isn't referenced by somebody else */
+	
+	if(!skb_cloned(skb)){
+		struct sk_buff *prev_skb = skb->prev;
+		int skb_size = skb->len;
+		int old_headlen = 0;
+		int ua_data = 0;
+		int uad_head = 0;
+		int uad_frags = 0;
+		int ua_nr_frags = 0;
+		int ua_frags_diff = 0;
+		
+		/*   Since this technique currently does not support SACK, I
+ *   		 * return -1 if the previous has been SACK'd. */
+		if(TCP_SKB_CB(prev_skb)->sacked & TCPCB_SACKED_ACKED){
+			return -1;
+		}
+		
+		/*   Current skb is out of window. */
+		if (after(TCP_SKB_CB(skb)->end_seq, tp->snd_una+tp->snd_wnd)){
+			return -1;
+		}
+		
+		/*  TODO: Optimize this part with regards to how the 
+ *  		  variables are initialized */
+		
+		/*  Calculates the ammount of unacked data that is available*/
+		ua_data = (TCP_SKB_CB(prev_skb)->end_seq - tp->snd_una > 
+			   prev_skb->len ? prev_skb->len : 
+			   TCP_SKB_CB(prev_skb)->end_seq - tp->snd_una);
+		ua_frags_diff = ua_data - prev_skb->data_len;
+		uad_frags = (ua_frags_diff > 0 ? prev_skb->data_len : ua_data);
+		uad_head = (ua_frags_diff > 0 ? ua_data - uad_frags : 0);
+
+		if(ua_data <= 0)
+			return -1;
+		
+		if(uad_frags > 0){
+			int i = 0;
+			int bytes_frags = 0;
+			
+			if(uad_frags == prev_skb->data_len){
+				ua_nr_frags = skb_shinfo(prev_skb)->nr_frags;
+			} else{
+				for(i=skb_shinfo(prev_skb)->nr_frags - 1; i>=0; i--){
+					if(skb_shinfo(prev_skb)->frags[i].size 
+					   + bytes_frags == uad_frags){
+						ua_nr_frags += 1;
+						break;
+					} 	  
+					ua_nr_frags += 1;
+					bytes_frags += skb_shinfo(prev_skb)->frags[i].size;
+				}
+			}
+		}
+		
+               if ((sysctl_tcp_rdb_max_bundle_bytes == 0 && ((skb_size + ua_data) > mss_now))
+                   || (sysctl_tcp_rdb_max_bundle_bytes > 0 && ((skb_size + ua_data) >
+                                                               sysctl_tcp_rdb_max_bundle_bytes))){
+                       return -1;
+               }
+
+		
+		/*   We need to know tailroom, even if it is nonlinear */
+		if(uad_head > (skb->end - skb->tail)){
+			return -1;
+		}
+		
+		if(skb_is_nonlinear(skb) && (uad_frags > 0)){
+			if((ua_nr_frags +
+			    skb_shinfo(skb)->nr_frags) > MAX_SKB_FRAGS){
+				return -1;
+			}
+			
+			if(skb_headlen(skb) > 0){
+				return -1;
+			}
+		}
+		
+		if((uad_frags > 0) && skb_headlen(skb) > 0){
+			return -1;
+		}
+		
+		/*   To avoid duplicate copies (and copies
+ *   		   where parts have been acked) */
+		if(TCP_SKB_CB(skb)->seq <= (TCP_SKB_CB(prev_skb)->end_seq - ua_data)){
+			return -1;
+		}
+		
+		/*  SYN's are holy*/
+		if(TCP_SKB_CB(skb)->flags & TCPCB_FLAG_SYN || TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN){
+			return -1;
+		}
+		
+		/*   Copy linear data */
+		if(uad_head > 0){
+			printk(KERN_INFO "冗余linear");
+			
+			/*   Add required space to the header. Can't use put due to linearity */
+			old_headlen = skb_headlen(skb);
+			skb->tail += uad_head;
+			skb->len += uad_head;
+			
+			if(skb_headlen(skb) > 0){
+				memmove(skb->data + uad_head, skb->data, old_headlen);
+			}
+			
+			skb_copy_to_linear_data(skb, prev_skb->data + (skb_headlen(prev_skb) - uad_head), uad_head);
+		}
+		
+		/*  Copy paged data*/
+		if(uad_frags > 0){
+			int i = 0;
+			/*  Must move data backwards in the array.*/
+			if(skb_is_nonlinear(skb)){
+				memmove(skb_shinfo(skb)->frags + ua_nr_frags,
+					skb_shinfo(skb)->frags,
+					skb_shinfo(skb)->nr_frags*sizeof(skb_frag_t));
+			}
+			
+			/*  Copy info and update pages*/
+			memcpy(skb_shinfo(skb)->frags,
+			       skb_shinfo(prev_skb)->frags + (skb_shinfo(prev_skb)->nr_frags - ua_nr_frags),
+			       ua_nr_frags*sizeof(skb_frag_t));
+			
+			for(i=0; i<ua_nr_frags;i++){
+				get_page(skb_shinfo(skb)->frags[i].page);
+			}
+			
+			skb_shinfo(skb)->nr_frags += ua_nr_frags;
+			skb->data_len += uad_frags;
+			skb->len += uad_frags;
+			printk(KERN_INFO "冗余 %d page\n",uad_frags);
+		}
+		
+		TCP_SKB_CB(skb)->seq = TCP_SKB_CB(prev_skb)->end_seq - ua_data;
+		
+		if(skb->ip_summed == CHECKSUM_PARTIAL)
+			skb->csum = CHECKSUM_PARTIAL;
+		else
+			skb->csum = skb_checksum(skb, 0, skb->len, 0);
+	}
+	
+	return 1;
+}
+
+
+/*hmy */
+
+
 int tcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
 		size_t size)
 {
@@ -1074,6 +1239,20 @@
 
 			from += copy;
 			copied += copy;
+
+
+	/*  Added at Simula to support RDB */
+			if(((tp->thin_rdb||sysctl_tcp_force_thin_rdb))&& skb->len < mss_now){
+				if(skb->prev != (struct sk_buff*) &(sk)->sk_write_queue
+				   && !(TCP_SKB_CB(skb)->flags & TCPCB_FLAG_SYN)
+				   && !(TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN)){
+					tcp_trans_merge_prev(sk, skb, mss_now);
+				}
+			} /*  End - Simula */ 
+
+
+
+
 			if ((seglen -= copy) == 0 && iovlen == 0)
 				goto out;
 
@@ -2258,6 +2437,13 @@
 			tp->nonagle &= ~TCP_NAGLE_OFF;
 		}
 		break;
+	/* hmy add */
+       case TCP_THIN_RDB:
+               if(val)
+                       tp->thin_rdb = 1;
+               break;
+
+	/* hmy add end */
 
 	case TCP_THIN_LINEAR_TIMEOUTS:
 		if (val < 0 || val > 1)
--- linux-2.6.35/net/ipv4/tcp_input.c	2012-03-09 11:02:20.153623891 +0800
+++ linux-2.6.35-rdb/net/ipv4/tcp_input.c	2012-03-14 10:43:34.157672361 +0800
@@ -3304,6 +3304,75 @@
 		if (skb == tp->lost_skb_hint)
 			tp->lost_skb_hint = NULL;
 	}
+//hmy add
+	do {
+		int remove = 0;
+		int i;
+		int remove_head = 0;
+		int remove_frags = 0;
+		int no_frags;
+		int data_frags;
+		struct sk_buff *skb1;
+		skb1 = tcp_send_head(sk);
+		struct tcp_skb_cb *scb = TCP_SKB_CB(skb1);
+
+		if (skb1 == NULL || tcp_write_queue_head == NULL) {
+			break;
+		}
+
+		if ((tp->thin_rdb || sysctl_tcp_force_thin_rdb)&& after(scb->end_seq, tp->snd_una)&& after(tp->snd_una, scb->seq)) {
+			remove = tp->snd_una - scb->seq;
+			remove_head =(remove >skb_headlen(skb1) ? skb_headlen(skb1) :remove);
+			remove_frags =(remove >skb_headlen(skb1) ? remove - remove_head : 0);
+
+			//linear remove
+			if (skb_headlen(skb1) > 0 && remove_head > 0) {
+				memmove(skb1->data,
+					skb1->data + remove_head,
+					skb_headlen(skb1) - remove_head);
+				skb1->tail -= remove_head;
+			printk(KERN_INFO "删除冗余linear");
+			}
+			//page remove
+			if (skb_is_nonlinear(skb1) && remove_frags > 0) {
+				no_frags = 0;
+				data_frags = 0;
+
+				/* Remove unecessary pages */
+				for (i = 0; i < skb_shinfo(skb1)->nr_frags;i++) {
+					if (data_frags +skb_shinfo(skb1)->frags[i].size == remove_frags) {
+						put_page(skb_shinfo(skb1)->frags[i].page);
+						no_frags += 1;
+						break;
+					}
+					put_page(skb_shinfo(skb1)->frags[i].page);
+					no_frags += 1;
+					data_frags +=skb_shinfo(skb1)->frags[i].size;
+				}
+
+				if (skb_shinfo(skb1)->nr_frags > no_frags)
+					memmove(skb_shinfo(skb1)->frags,
+						skb_shinfo(skb1)->frags +no_frags,
+						(skb_shinfo(skb1)->nr_frags -no_frags) *sizeof(skb_frag_t));
+
+				skb1->data_len -= remove_frags;
+				skb_shinfo(skb1)->nr_frags -= no_frags;
+			printk(KERN_INFO "删除冗余 %d page",remove_frags);
+
+			}
+
+		        scb->seq += remove;
+			skb1->len -= remove;
+                                       if(skb1->ip_summed == CHECKSUM_PARTIAL)
+                                               skb1->csum = CHECKSUM_PARTIAL;
+                                       else
+                                               skb1->csum = skb_checksum(skb1, 0, skb1->len, 0);
+
+
+		}
+}while (0);
+		//add end
+
 
 	if (likely(between(tp->snd_up, prior_snd_una, tp->snd_una)))
 		tp->snd_up = tp->snd_una;
--- linux-2.6.35/net/ipv4/tcp_output.c	2012-03-08 15:58:37.000000000 +0800
+++ linux-2.6.35-rdb/net/ipv4/tcp_output.c	2012-03-13 15:20:45.807025486 +0800
@@ -2012,6 +2012,170 @@
 	return 1;
 }
 
+//hmy add
+/* Added at Simula. Variation of the regular collapse,
+   adapted to support RDB  */
+static void tcp_retrans_merge_redundant(struct sock *sk,
+					struct sk_buff *skb,
+					int mss_now)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct sk_buff *next_skb = skb->next;
+	int skb_size = skb->len;
+	int new_data = 0;
+	int new_data_head = 0;
+	int new_data_frags = 0;
+	int new_frags = 0;
+	int old_headlen = 0;
+	
+	int i;
+	int data_frags = 0;
+	printk(KERN_INFO "重传时冗余");
+	
+	/* Loop through as many packets as possible
+	 * (will create a lot of redundant data, but WHATEVER).
+	 * The only packet this MIGHT be critical for is
+	 * if this packet is the last in the retrans-queue.
+	 *
+	 * Make sure that the first skb isnt already in
+	 * use by somebody else. */
+	
+	if (!skb_cloned(skb)) {
+		/* Iterate through the retransmit queue */
+		for (; (next_skb != (sk)->sk_send_head) && 
+			     (next_skb != (struct sk_buff *) &(sk)->sk_write_queue); 
+		     next_skb = next_skb->next) {
+			
+			/* Reset variables */
+			new_frags = 0;
+			data_frags = 0;
+			new_data = TCP_SKB_CB(next_skb)->end_seq - TCP_SKB_CB(skb)->end_seq;
+			
+			/* New data will be stored at skb->start_add + some_offset, 
+			   in other words the last N bytes */
+			new_data_frags = (new_data > next_skb->data_len ? 
+					  next_skb->data_len : new_data);
+			new_data_head = (new_data > next_skb->data_len ? 
+					 new_data - skb->data_len : 0);
+			
+			/*
+			 * 1. Contains the same data
+			 * 2. Size
+			 * 3. Sack
+			 * 4. Window
+			 * 5. Cannot merge with a later packet that has linear data
+			 * 6. The new number of frags will exceed the limit
+			 * 7. Enough tailroom
+			 */
+			
+			if(new_data <= 0){
+				return;
+			}
+			
+			if ((sysctl_tcp_rdb_max_bundle_bytes == 0 && ((skb_size + new_data) > mss_now))
+			    || (sysctl_tcp_rdb_max_bundle_bytes > 0 && ((skb_size + new_data) >
+									sysctl_tcp_rdb_max_bundle_bytes))){
+				return;
+			}
+			
+			if(TCP_SKB_CB(next_skb)->flags & TCPCB_FLAG_FIN){
+				return;
+			}
+			
+			if((TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED) ||
+			   (TCP_SKB_CB(next_skb)->sacked & TCPCB_SACKED_ACKED)){
+				return;
+			}
+			
+			if(after(TCP_SKB_CB(skb)->end_seq + new_data, tp->snd_una + tp->snd_wnd)){
+				return;
+			}
+			
+			if(skb_shinfo(skb)->frag_list || skb_shinfo(skb)->frag_list){
+				return;
+			}
+			
+			/* Calculate number of new fragments. Any new data will be 
+			   stored in the back. */
+			if(skb_is_nonlinear(next_skb)){
+				i = (skb_shinfo(next_skb)->nr_frags == 0 ? 
+				     0 : skb_shinfo(next_skb)->nr_frags - 1);
+				for( ; i>=0;i--){
+					if(data_frags + skb_shinfo(next_skb)->frags[i].size == 
+					   new_data_frags){
+						new_frags += 1;
+						break;
+					}
+					
+					data_frags += skb_shinfo(next_skb)->frags[i].size;
+					new_frags += 1;
+				}
+			}
+			
+			/* If dealing with a fragmented skb, only merge 
+			   with an skb that ONLY contain frags */
+			if(skb_is_nonlinear(skb)){
+				
+				/*Due to the way packets are processed, no later data*/
+				if(skb_headlen(next_skb) && new_data_head > 0){
+					return;
+				}
+				
+				if(skb_is_nonlinear(next_skb) && (new_data_frags > 0) && 
+				   ((skb_shinfo(skb)->nr_frags + new_frags) > MAX_SKB_FRAGS)){
+					return;
+				}
+				
+			} else {
+				if(skb_headlen(next_skb) && (new_data_head > (skb->end - skb->tail))){
+					return;
+				}
+			}
+			
+			/*Copy linear data. This will only occur if both are linear, 
+			  or only A is linear*/
+			if(skb_headlen(next_skb) && (new_data_head > 0)){
+				old_headlen = skb_headlen(skb);
+				skb->tail += new_data_head;
+				skb->len += new_data_head;
+				
+				/* The new data starts in the linear area, 
+				   and the correct offset will then be given by 
+				   removing new_data ammount of bytes from length. */
+				skb_copy_to_linear_data_offset(skb, old_headlen, next_skb->tail - 
+							       new_data_head, new_data_head);
+			}
+			
+			if(skb_is_nonlinear(next_skb) && (new_data_frags > 0)){
+				memcpy(skb_shinfo(skb)->frags + skb_shinfo(skb)->nr_frags, 
+				       skb_shinfo(next_skb)->frags + 
+				       (skb_shinfo(next_skb)->nr_frags - new_frags), 
+				       new_frags*sizeof(skb_frag_t));
+				
+				for(i=skb_shinfo(skb)->nr_frags; 
+				    i < skb_shinfo(skb)->nr_frags + new_frags; i++)
+					get_page(skb_shinfo(skb)->frags[i].page);
+				
+				skb_shinfo(skb)->nr_frags += new_frags;
+				skb->data_len += new_data_frags;
+				skb->len += new_data_frags;
+			}
+			
+			TCP_SKB_CB(skb)->end_seq += new_data;
+						
+			if(skb->ip_summed == CHECKSUM_PARTIAL)
+				skb->csum = CHECKSUM_PARTIAL;
+			else
+				skb->csum = skb_checksum(skb, 0, skb->len, 0);
+			
+			skb_size = skb->len;
+		}
+		
+	}
+}
+
+
+//hmy add end
 /* Collapse packets in the retransmit queue to make to create
  * less packets on the wire. This is only done on retransmission.
  */
@@ -2111,6 +2275,18 @@
 
 	tcp_retrans_try_collapse(sk, skb, cur_mss);
 
+//hmy add
+	 if ((tp->thin_rdb || sysctl_tcp_force_thin_rdb)) {
+               if (!(TCP_SKB_CB(skb)->flags & TCPCB_FLAG_SYN) &&
+                   !(TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN) &&
+                   (skb->next != tcp_send_head(sk)) &&
+                   (skb->next != (struct sk_buff *) &sk->sk_write_queue)) {
+                       tcp_retrans_merge_redundant(sk, skb, cur_mss);
+               }
+
+}
+//add end
+
 	/* Some Solaris stacks overoptimize and ignore the FIN on a
 	 * retransmit when old data is attached.  So strip it off
 	 * since it is cheap to do so and saves bytes on the network.
